{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optuna AllenNLP custom loop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTyp1L/zXvWOFF1/EdtVKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himkt/optuna-allennlp/blob/master/Optuna_AllenNLP_custom_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6pVEnVBttn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "23446390-2a04-494e-d470-d7ff59256f50"
      },
      "source": [
        "!pip install --quiet \"allennlp==v1.1.0rc3\" optuna"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 491kB 12.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 778kB 52.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 60.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 56.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.3MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 51.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 65.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 60.2MB/s \n",
            "\u001b[?25h  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kooGmC1a1_Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "from allennlp.data import Vocabulary, allennlp_collate\n",
        "from allennlp.data.dataset_readers import TextClassificationJsonReader\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import WhitespaceTokenizer\n",
        "from allennlp.models import BasicClassifier\n",
        "from allennlp.modules import Embedding\n",
        "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.training import GradientDescentTrainer\n",
        "import numpy\n",
        "import optuna\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from optuna.integration import AllenNLPPruningCallback\n",
        "from optuna import Trial"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2QUP8_N2FQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data():\n",
        "    reader = TextClassificationJsonReader(\n",
        "        token_indexers={\"tokens\": SingleIdTokenIndexer()},\n",
        "        tokenizer=WhitespaceTokenizer(),\n",
        "    )\n",
        "    train_dataset = reader.read(\"https://s3-us-west-2.amazonaws.com/allennlp/datasets/imdb/train.jsonl\")  # NOQA\n",
        "    valid_dataset = reader.read(\"https://s3-us-west-2.amazonaws.com/allennlp/datasets/imdb/dev.jsonl\")  # NOQA\n",
        "    vocab = Vocabulary.from_instances(train_dataset)\n",
        "    train_dataset.index_with(vocab)\n",
        "    valid_dataset.index_with(vocab)\n",
        "    return train_dataset, valid_dataset, vocab"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09YvXjXI2I6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(\n",
        "        vocab: Vocabulary,\n",
        "        embedding_dim: int,\n",
        "        max_filter_size: int,\n",
        "        num_filters: int,\n",
        "        output_dim: int,\n",
        "        dropout: float,\n",
        "):\n",
        "    model = BasicClassifier(\n",
        "        text_field_embedder=BasicTextFieldEmbedder(\n",
        "            {\n",
        "                \"tokens\": Embedding(\n",
        "                  embedding_dim=embedding_dim,\n",
        "                  trainable=True,\n",
        "                  vocab=vocab\n",
        "              )\n",
        "            }\n",
        "        ),\n",
        "        seq2vec_encoder=CnnEncoder(\n",
        "            ngram_filter_sizes=range(2, max_filter_size),\n",
        "            num_filters=num_filters,\n",
        "            embedding_dim=embedding_dim,\n",
        "            output_dim=output_dim,\n",
        "        ),\n",
        "        dropout=dropout,\n",
        "        vocab=vocab,\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxODxVjU2LhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective(trial: Trial):\n",
        "    embedding_dim = trial.suggest_int(\"embedding_dim\", 128, 256)\n",
        "    max_filter_size = trial.suggest_int(\"max_filter_size\", 3, 6)\n",
        "    num_filters = trial.suggest_int(\"num_filters\", 128, 256)\n",
        "    output_dim = trial.suggest_int(\"output_dim\", 128, 512)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0, 1.0)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
        "\n",
        "    train_dataset, valid_dataset, vocab = prepare_data()\n",
        "    model = build_model(vocab, embedding_dim, max_filter_size, num_filters, output_dim, dropout)\n",
        "    model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    optimizer = SGD(model.parameters(), lr=lr)\n",
        "    data_loader = DataLoader(train_dataset, batch_size=10, collate_fn=allennlp_collate)\n",
        "    validation_data_loader = DataLoader(valid_dataset, batch_size=64, collate_fn=allennlp_collate)\n",
        "    trainer = GradientDescentTrainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=data_loader,\n",
        "        validation_data_loader=validation_data_loader,\n",
        "        validation_metric=\"+accuracy\",\n",
        "        patience=None,  # `patience=None` since it could conflict with AllenNLPPruningCallback\n",
        "        num_epochs=10,\n",
        "        serialization_dir=f\"result/{trial.number}\",\n",
        "        epoch_callbacks=[AllenNLPPruningCallback(trial, \"validation_accuracy\")],\n",
        "    )\n",
        "    return trainer.train()[\"best_validation_accuracy\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q14wrX0I2OX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "f4f07afb-bc4b-4c3f-f23c-9648ad4d9f85"
      },
      "source": [
        "random.seed(41)\n",
        "torch.manual_seed(41)\n",
        "numpy.random.seed(41)\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.HyperbandPruner(),\n",
        ")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading instances: 0it [00:00, ?it/s]\n",
            "downloading:   0%|          | 0/26939140 [00:00<?, ?B/s]\n",
            "downloading:   0%|          | 17408/26939140 [00:00<04:05, 109854.47B/s]\n",
            "downloading:   0%|          | 52224/26939140 [00:00<03:29, 128270.00B/s]\n",
            "downloading:   0%|          | 104448/26939140 [00:00<02:51, 156103.41B/s]\n",
            "downloading:   1%|          | 243712/26939140 [00:00<02:09, 206601.87B/s]\n",
            "downloading:   2%|1         | 522240/26939140 [00:00<01:34, 280373.97B/s]\n",
            "downloading:   4%|3         | 1061888/26939140 [00:00<01:06, 386318.59B/s]\n",
            "downloading:   8%|8         | 2158592/26939140 [00:01<00:46, 538457.91B/s]\n",
            "downloading:  16%|#6        | 4365312/26939140 [00:01<00:29, 756155.51B/s]\n",
            "downloading:  24%|##4       | 6577152/26939140 [00:01<00:19, 1054665.82B/s]\n",
            "downloading:  33%|###2      | 8756224/26939140 [00:01<00:12, 1456691.48B/s]\n",
            "downloading:  41%|####      | 10935296/26939140 [00:01<00:08, 1986769.02B/s]\n",
            "downloading:  49%|####8     | 13097984/26939140 [00:01<00:05, 2664945.68B/s]\n",
            "downloading:  57%|#####6    | 15293440/26939140 [00:02<00:03, 3505394.52B/s]\n",
            "downloading:  65%|######5   | 17570816/26939140 [00:02<00:02, 4515306.75B/s]\n",
            "downloading:  73%|#######3  | 19749888/26939140 [00:02<00:01, 5625088.80B/s]\n",
            "downloading:  82%|########1 | 21994496/26939140 [00:02<00:00, 6823432.93B/s]\n",
            "downloading:  90%|########9 | 24173568/26939140 [00:02<00:00, 7977640.01B/s]\n",
            "downloading: 100%|##########| 26939140/26939140 [00:02<00:00, 9040240.91B/s]\n",
            "reading instances: 20000it [00:13, 1452.83it/s]\n",
            "reading instances: 0it [00:00, ?it/s]\n",
            "downloading:   0%|          | 0/6810733 [00:00<?, ?B/s]\n",
            "downloading:   0%|          | 1024/6810733 [00:00<17:44, 6395.05B/s]\n",
            "downloading:   1%|          | 34816/6810733 [00:00<12:31, 9014.77B/s]\n",
            "downloading:   1%|1         | 87040/6810733 [00:00<08:48, 12722.49B/s]\n",
            "downloading:   3%|2         | 191488/6810733 [00:00<06:07, 18019.29B/s]\n",
            "downloading:   6%|6         | 417792/6810733 [00:00<04:09, 25597.32B/s]\n",
            "downloading:  13%|#2        | 870400/6810733 [00:00<02:43, 36421.47B/s]\n",
            "downloading:  25%|##4       | 1688576/6810733 [00:01<01:38, 51866.88B/s]\n",
            "downloading:  50%|####9     | 3398656/6810733 [00:01<00:46, 73936.32B/s]\n",
            "downloading: 100%|##########| 6810733/6810733 [00:01<00:00, 4539081.80B/s]\n",
            "reading instances: 5000it [00:05, 913.77it/s]\n",
            "building vocab: 100%|##########| 20000/20000 [00:02<00:00, 6784.98it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: ExperimentalWarning:\n",
            "\n",
            "AllenNLPPruningCallback is experimental (supported from v2.0.0). The interface can change in the future.\n",
            "\n",
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "accuracy: 0.4970, batch_loss: 0.6988, loss: 0.6933 ||: 100%|##########| 2000/2000 [00:41<00:00, 47.96it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6898, loss: 0.6932 ||: 100%|##########| 79/79 [00:03<00:00, 21.68it/s]\n",
            "accuracy: 0.4956, batch_loss: 0.6955, loss: 0.6933 ||: 100%|##########| 2000/2000 [00:38<00:00, 52.32it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6897, loss: 0.6932 ||: 100%|##########| 79/79 [00:02<00:00, 27.54it/s]\n",
            "accuracy: 0.4988, batch_loss: 0.6941, loss: 0.6933 ||: 100%|##########| 2000/2000 [00:37<00:00, 52.88it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6896, loss: 0.6931 ||: 100%|##########| 79/79 [00:02<00:00, 27.39it/s]\n",
            "accuracy: 0.5014, batch_loss: 0.6941, loss: 0.6932 ||: 100%|##########| 2000/2000 [00:37<00:00, 52.77it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6898, loss: 0.6931 ||: 100%|##########| 79/79 [00:02<00:00, 27.41it/s]\n",
            "accuracy: 0.5041, batch_loss: 0.6945, loss: 0.6931 ||: 100%|##########| 2000/2000 [00:37<00:00, 52.85it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6896, loss: 0.6931 ||: 100%|##########| 79/79 [00:02<00:00, 27.50it/s]\n",
            "accuracy: 0.4999, batch_loss: 0.6913, loss: 0.6932 ||: 100%|##########| 2000/2000 [00:37<00:00, 52.81it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6897, loss: 0.6931 ||: 100%|##########| 79/79 [00:02<00:00, 27.33it/s]\n",
            "accuracy: 0.5052, batch_loss: 0.6927, loss: 0.6930 ||: 100%|##########| 2000/2000 [00:37<00:00, 52.83it/s]\n",
            "accuracy: 0.5000, batch_loss: 0.6896, loss: 0.6931 ||: 100%|##########| 79/79 [00:02<00:00, 27.30it/s]\n",
            "accuracy: 0.5061, batch_loss: 0.6895, loss: 0.6931 ||:  60%|######    | 1208/2000 [00:22<00:15, 52.06it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
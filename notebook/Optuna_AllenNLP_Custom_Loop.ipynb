{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/himkt/optuna-allennlp/blob/master/notebook/Optuna_AllenNLP_Custom_Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tB6pVEnVBttn"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet \"allennlp==v1.1.0rc3\" optuna plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kooGmC1a1_Sm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from allennlp.data import Vocabulary, allennlp_collate\n",
    "from allennlp.data.dataset_readers import TextClassificationJsonReader\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import WhitespaceTokenizer\n",
    "from allennlp.models import BasicClassifier\n",
    "from allennlp.modules import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "import numpy\n",
    "import optuna\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from optuna.integration import AllenNLPPruningCallback\n",
    "from optuna import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2QUP8_N2FQQ"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    reader = TextClassificationJsonReader(\n",
    "        token_indexers={\"tokens\": SingleIdTokenIndexer()},\n",
    "        tokenizer=WhitespaceTokenizer(),\n",
    "    )\n",
    "    train_dataset = reader.read(\"https://s3-us-west-2.amazonaws.com/allennlp/datasets/imdb/train.jsonl\")  # NOQA\n",
    "    valid_dataset = reader.read(\"https://s3-us-west-2.amazonaws.com/allennlp/datasets/imdb/dev.jsonl\")  # NOQA\n",
    "    vocab = Vocabulary.from_instances(train_dataset)\n",
    "    train_dataset.index_with(vocab)\n",
    "    valid_dataset.index_with(vocab)\n",
    "    return train_dataset, valid_dataset, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09YvXjXI2I6w"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "        vocab: Vocabulary,\n",
    "        embedding_dim: int,\n",
    "        max_filter_size: int,\n",
    "        num_filters: int,\n",
    "        output_dim: int,\n",
    "        dropout: float,\n",
    "):\n",
    "    model = BasicClassifier(\n",
    "        text_field_embedder=BasicTextFieldEmbedder(\n",
    "            {\n",
    "                \"tokens\": Embedding(\n",
    "                  embedding_dim=embedding_dim,\n",
    "                  trainable=True,\n",
    "                  vocab=vocab\n",
    "              )\n",
    "            }\n",
    "        ),\n",
    "        seq2vec_encoder=CnnEncoder(\n",
    "            ngram_filter_sizes=range(2, max_filter_size),\n",
    "            num_filters=num_filters,\n",
    "            embedding_dim=embedding_dim,\n",
    "            output_dim=output_dim,\n",
    "        ),\n",
    "        dropout=dropout,\n",
    "        vocab=vocab,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxODxVjU2LhX"
   },
   "outputs": [],
   "source": [
    "def objective(trial: Trial):\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 128, 256)\n",
    "    max_filter_size = trial.suggest_int(\"max_filter_size\", 3, 6)\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 128, 256)\n",
    "    output_dim = trial.suggest_int(\"output_dim\", 128, 512)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1.0)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "    train_dataset, valid_dataset, vocab = prepare_data()\n",
    "    model = build_model(vocab, embedding_dim, max_filter_size, num_filters, output_dim, dropout)\n",
    "    model.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    data_loader = DataLoader(train_dataset, batch_size=10, collate_fn=allennlp_collate)\n",
    "    validation_data_loader = DataLoader(valid_dataset, batch_size=64, collate_fn=allennlp_collate)\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=data_loader,\n",
    "        validation_data_loader=validation_data_loader,\n",
    "        validation_metric=\"+accuracy\",\n",
    "        patience=None,  # `patience=None` since it could conflict with AllenNLPPruningCallback\n",
    "        num_epochs=10,\n",
    "        serialization_dir=f\"result/{trial.number}\",\n",
    "        epoch_callbacks=[AllenNLPPruningCallback(trial, \"validation_accuracy\")],\n",
    "    )\n",
    "    return trainer.train()[\"best_validation_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q14wrX0I2OX4"
   },
   "outputs": [],
   "source": [
    "random.seed(41)\n",
    "torch.manual_seed(41)\n",
    "numpy.random.seed(41)\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.HyperbandPruner(),\n",
    ")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RsHeYY18mFQ"
   },
   "outputs": [],
   "source": [
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlCI8hCO8qR8"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqfDUFDi8snd"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPdsdYSAPHk/l1horfomMme",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Optuna AllenNLP Custom Loop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
